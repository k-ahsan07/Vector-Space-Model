########################   UTILITIES        ########################################
import nltk
from nltk import word_tokenize
from nltk.stem import PorterStemmer
import json
import os
from sys import argv
import time
import numpy as np
import sklearn
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from queue import PriorityQueue
nltk.download('punkt')



#load stopwords to eliminate them while processing
file = open("Stopword-List.txt","r")
content = file.read()
file.close()
stopWords = content.split()



########################   FUNCTIONS           ########################################


def stopWordsRemoval(tokens):
    #return all tokens which are not part of stopWords
    words = []
    for token in tokens:
        if token not in stopWords:
            words.append(token)
    return words


def case_fold(token):
    #convert token to lower case to normalize it
    token = token.lower()
    word = token
    token = ""
    #only conisder english alphabets in the token, not special characters
    for letter in word:
        if letter >= 'a' and letter <= 'z':
            token = token + letter
    return token


def stem(token):
    #use porter stemmer to stem the token
    stemmer = PorterStemmer()
    stemmedWord = stemmer.stem(token)
    return stemmedWord


def normalize(token):
    #apply case folding
    token = case_fold(token)
    #apply stemming
    token = stem(token)
    return token


def tokenizer(content):
    #extract different words from the string
    content = content.replace("-", " ")
    content = content.replace("â€¢", " ")
    words = word_tokenize(content)
    return words





def parseDoc(content):
    words = tokenizer(content)
    tokens = []
    for word in words:
        #apply normalization to token
        token = normalize(word)
        #make sure there are no empty spaces in the tokens list
        if len(token) > 0:
            tokens.append(token)
    #remove stopwords from list
    tokens = stopWordsRemoval(tokens)
    return tokens




def parseQuery(query):
    #tokenize the query and return tokens
    words = tokenizer(query)
    tokens = []
    for word in words:
        #apply normalization to token
        token = normalize(word)
        #make sure there are no empty spaces in the tokens list
        if len(token) > 0:
            tokens.append(token)
    #remove stopwords from list
    tokens = stopWordsRemoval(tokens)
    return " ".join(tokens)



def makeCorpus(data_dir="Dataset"):
    corpus = [] # corpus generated by all documents
    # Iterate over all files in the specified directory
    for filename in os.listdir(data_dir):
        # Construct the file path
        filePath = os.path.join(data_dir, filename)
        # Check if the item in the directory is a file
        if os.path.isfile(filePath):
            # Open the document and read its content
            with open(filePath, "r") as doc:
                docContent = doc.read()
            # Parse the document content and get tokens
            tokens = parseDoc(docContent)
            # Generate corpus by joining all tokens with a space separating them and append the string to the list
            corpus.append(" ".join(tokens))
    return corpus



def saveVocabList(vocab):
    #write the vocabulary to json file
    with open("vocabulary.json","w") as file:
        json.dump(vocab,file)
    

def loadVocabList():
    #load vocab from file
    with open("vocabulary.json", "r") as file:
        vocabList = json.load(file)
    vocab = np.array(vocabList)
    return vocab


def saveNormalizedVectors(normVecs):
    #write the vectors to json file
    with open("normalizedVectors.json","w") as file:
        json.dump(normVecs,file)
    

def loadNormalizedVectors():
    #load vectors from file
    with open("normalizedVectors.json", "r") as file:
        normVecs = json.load(file)
    normVecs = np.array(normVecs)
    return normVecs



def createVectors(corpus):
    # create an instance of TfidfVectorizer
    vectorizer = TfidfVectorizer()
    # fit the vectorizer on the corpus
    vectors = vectorizer.fit_transform(corpus)
    # get the vocabulary (the list of unique words in the corpus)
    vocab = vectorizer.get_feature_names_out(corpus)
    #save vocabulary list
    saveVocabList(vocab.tolist())
    return vectors


def createQueryVector(query): 
    vocab = loadVocabList()
    # create an instance of TfidfVectorizer
    vectorizer = TfidfVectorizer(vocabulary= vocab)
    # fit the vectorizer on the corpus
    vectors = vectorizer.fit_transform([query]).toarray()
    return vectors[0]



def normalizeVectors(vectors):
    # Normalize the vectors using L2 normalization
    normalized_vectors = sklearn.preprocessing.normalize(vectors, norm='l2').toarray()
    return normalized_vectors



def computeScores(query):
    #create PriorityQueue to order documents by their scores
    pq = PriorityQueue()
    #get all document vectors
    docVectors = loadNormalizedVectors()
    for i in range(0,20,1):
        doc = docVectors[i]
        #get common indices between query and doc and perform doc product on those indices
        commonIndices = np.intersect1d(np.nonzero(query), np.nonzero(doc))
        score = np.dot(query[commonIndices], doc[commonIndices])
        #add (score,docNo) tuple to priorityQueue
        pq.put((score, i+1))
    #pop docs in reverse order to get highest score first
    scores = []
    for i in range(0,20,1):
        scores.append(pq.get())
    return scores



def getResult(scores):
    result = []
    #set threshold
    a = 0.035
    #append documents with score > threshold to result set
    #compare in opposite order since scores are sorted in ascending order
    for i in range(0,20,1):
        if scores[19-i][0] > a:
            result.append(scores[19-i][1])
    
    return result


########################   DOCUMENT PROCESSING    ########################################




def processDocs():
    #get corpus of all docs
    corpus = makeCorpus()
    #create document vectors
    vectors = createVectors(corpus)
    #normalize document vectors
    normalized_vectors = normalizeVectors(vectors)
    #save normalized vectors for uSe later
    saveNormalizedVectors(normalized_vectors.tolist())





########################    QUERY PROCESSING ########################################






def processQuery(query):
    # Record the start time
    start_time = time.time()
    #Parse query
    query = parseQuery(query)
    #create query vector
    vector = createQueryVector(query)
    #compute similarity scores between all documents and query
    scores = computeScores(vector)
    #get sorted result list
    result = getResult(scores)
    # Record the end time
    end_time = time.time()
    # Calculate the time elapsed
    time_taken = end_time - start_time
    return result, time_taken